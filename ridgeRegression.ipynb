{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Our goal is to learn about Ridge Regression. \n",
    "This technique is closely related to standard linear regression, but adds an extra twist.\n",
    "The twist involves including an additional term in our minimization of sums of squares that encourages coefficients to shrink.\n",
    "\n",
    "The idea behind ridge regression is that influencing coefficients to shrink towards zero will also discourage overfitting to our training data.\n",
    "If we can prevent overfitting than we can better generalize our model to yet uncollected data.\n",
    "\n",
    "\n",
    "## Mechanics\n",
    "\n",
    "We saw in previous lectures that finding optimal coefficients in a linear regression is the same as minimizing the sum squares error.\n",
    "More concretely, give a set of observations $(x,y)_{1},(x,y)_{2},\\cdots,(x,y)_{N}$ we can write our probabilistic model as \n",
    "\n",
    "$$\n",
    "    p(y|x) \\sim N( \\beta_{0} + \\beta'x , \\sigma^2)\n",
    "$$\n",
    "\n",
    "where $\\beta_{0}$ is an intercept.\n",
    "\n",
    "The optimal $\\beta$ parameters are found by minimizing the following function of $\\beta$\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^{N} \\left( y_{i} - \\beta_{0} - \\beta'x_{i} \\right)^{2}  \\right\\}\n",
    "$$\n",
    "\n",
    "Ridge regression adds an additional term to the above optimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^{N} \\left( y_{i} - \\beta_{0} - \\beta'x_{i} \\right)^{2}  \\right\\} + \\lambda \\sum_{m=1}^{M} \\beta_{m}^{2}\n",
    "$$\n",
    "\n",
    "The $\\lambda$ parameter is a choice on part of the investigator.\n",
    "Typically, $\\lambda$ is chosen by cross-validation.\n",
    "\n",
    "This additional term penalizes coefficients related to covariates. \n",
    "Ridge regression does **not** penalize the size of the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal $\\beta$\n",
    "\n",
    "We can solve the above by taking partial derivatives and finding the $\\beta$ that zeros out these derivatives.\n",
    "\n",
    "$$\n",
    "f(\\beta) = \\sum_{i=1}^{N} \\left( y_{i} - \\beta_{0} - \\beta'x_{i} \\right)^{2} + \\lambda \\sum_{m=1}^{M} \\beta_{m}^{2}\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "f(\\beta) = \\sum_{i=1}^{N} \\left( y_{i} - \\beta_{0} - \\beta'x_{i} \\right)^{2} + \\lambda \\sum_{m=1}^{M} \\beta_{m}^{2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
